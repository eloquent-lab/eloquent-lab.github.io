<html>
  <head>
    <meta charset="UTF-8">
    <title>ELOQUENT evaluation lab at CLEF</title>
  </head>
  <body>
  <body style="background-color:rgb(155, 155, 233);color: rgb(0, 0, 0); padding: 20px; font-family: 'Verdana', sans-serif;">
    <div style="padding:10px 200px; border: 5px solid gold;border-radius: 20px;">
      <img src="" alt="Moose" style="height: 140px; width: 240px; border-radius: 50%;">
    <h1 style="color: azure">
      ELOQUENT --- quality of generative language models
    </h1>
    <p>
      ELOQUENT is an evaluation lab with a set of shared tasks for evaluating the quality of generative language models. It will be launched in early 2024 as part of the <a href="http://clef2024.clef-initiative.eu/index.php">CLEF 2024</a> campaign. The lab will be presented at the <a href="https://clef2023.clef-initiative.eu/index.php">2023 CLEF</a> conference and at the <a href="https://www.ecir2024.org/">2024 ECIR</a> conference.
    </p>
    <p>
      ELOQUENT is planned to run for three years, with the first year more exploratory and the two following years in a more consolidated manner. We are very interested in getting in touch with potential participants early on in the process to help formulate and fine-tune the tasks to achieve both feasibility and external impact.
    </p>
    <h2>Proposed tasks</h2>
    <p>
      The evaluation tasks are planned to involve comparatively little human effort, and instead leverage the capacity of the current generation of AI models to generate, process, and assess input in the form of human language. For this first year we propose four tasks
    </p><p>
      The organisers wish to encourage efforts in multilingual research and as such do not intend to place any restrictions on the languages that participants may work with. For tasks in which interaction between models is required, models developed for the same language may be paired together. In cases where this is not possible, the application of automated translation will be considered as a bridging mechanism. These design choices will be discussed among participants before the final version of the tasks is launched.
      </p><p>
      <h3>Task 1: Topical Test Generation</h3>
      This task will test and verify a model's understanding of an application domain of interest through the application of automatically generated tests of domain knowledge. 
      <h3>Task 2: Veracity Detection</h3>
      This task will test how the truthfulness or veracity of automatically generated text can be assessed. 

      <h3>Task 3: Robustness</h3>
      This task is intended to test the capability of a model to handle dialectal, sociolectal, and cross-cultural variation as represented by human-generated varieties of input prompts. The results will be assessed by how variation in output is conditioned on variation of equivalent but non-identical input prompts.

      <h3>Voight-Kampff task</h3>
      This task aims to explore whether automatically-generated text can be distinguished from human-authored text. This task will be organised in collaboration with the PAN lab at CLEF.
    </p>

    <h2>Organisers</h2>
    <ul>
      <li>   Jussi Karlgren, Aarne Talman, Silo AI </li>
      <li>   Liane Guillou, Luise DÃ¼rlich, Evangelia Gogoulou, Joakim Nivre, RISE ICT </li>
      <li>   Magnus Sahlgren, AI Sweden </li>
    </ul>
</div> 
  </body>
</html>
